{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_columns import *\n",
    "import datetime\n",
    "import os\n",
    "from url_getter import get_sorted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_values(series):\n",
    "    return ' '.join(series.dropna().astype(str))\n",
    "\n",
    "def remove_duplicates_from_string(s):\n",
    "    return list(sorted(set(s.split()), key=s.split().index))\n",
    "\n",
    "def remove_duplicates_concatenate(df, columns):\n",
    "    df = df.groupby('source_url', as_index=False).agg({\n",
    "        col: concatenate_values for col in columns\n",
    "    })\n",
    "\n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(remove_duplicates_from_string)\n",
    "\n",
    "    return df\n",
    "\n",
    "def event_to_description(codes_list, code_dictionary):\n",
    "    descriptions = []\n",
    "    for code in codes_list:\n",
    "        # Check if the code is alphanumeric\n",
    "        if code.isalnum():\n",
    "            # Optionally, handle numeric or mixed alphanumeric codes\n",
    "            descriptions.append(code_dictionary.get(code, ''))\n",
    "        else:\n",
    "            # Skip or handle non-alphanumeric codes\n",
    "            continue  # or descriptions.append('Invalid code')\n",
    "    return ' '.join(descriptions)\n",
    "\n",
    "def actor_to_description(codes_list, code_dictionary):\n",
    "    descriptions = []\n",
    "    for code in codes_list:\n",
    "        if len(code) % 3 == 0:\n",
    "            chunks = [code[i:i+3] for i in range(0, len(code), 3)]\n",
    "            chunk_descriptions = [code_dictionary.get(chunk, '') for chunk in chunks]\n",
    "            descriptions.append(' '.join(chunk_descriptions))\n",
    "        else:\n",
    "            continue\n",
    "    return ' '.join(descriptions)\n",
    "\n",
    "def remove_duplicates_description(text):\n",
    "    words = text.split()\n",
    "    seen = set()\n",
    "    unique_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in seen:\n",
    "            seen.add(word)\n",
    "            unique_words.append(word)\n",
    "    \n",
    "    return ' '.join(unique_words)\n",
    "\n",
    "# Definir la funci√≥n para calcular los promedios\n",
    "def clean_dataframe(df):\n",
    "    clean_data = df.groupby('source_url', as_index=False).agg({\n",
    "        'num_sources': 'mean',\n",
    "        'avg_tone': 'mean',\n",
    "        'goldstein_scale' : 'mean',\n",
    "        'num_articles': 'mean',\n",
    "        'num_mentions': 'mean'\n",
    "    })\n",
    "    return clean_data\n",
    "\n",
    "def add_description(df, actor_codes, event_codes, code_dictionary):\n",
    "    # Initialize the descriptions column\n",
    "    df['description'] = ''\n",
    "\n",
    "    # Add descriptions for actor codes\n",
    "    for column in actor_codes:\n",
    "        df['description'] += df[column].apply(lambda x: actor_to_description(x, code_dictionary)) + ' '\n",
    "\n",
    "    # Add descriptions for event codes\n",
    "    for column in event_codes:\n",
    "        df['description'] += df[column].apply(lambda x: event_to_description(x, code_dictionary)) + ' '\n",
    "\n",
    "    df['description'] = df['description'].apply(remove_duplicates_description)\n",
    "\n",
    "    # Get the sorted data\n",
    "    sorted_data = get_sorted_data(df,int((len(df)*0.15)))\n",
    "    sorted_data\n",
    "    \n",
    "    return sorted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_codes = ['actor1_code', 'actor1_known_group_code', 'actor1_type1_code', \n",
    "         'actor2_code', 'actor2_known_group_code', 'actor2_type1_code']\n",
    "\n",
    "event_codes = ['event_code']\n",
    "\n",
    "important_columns = actor_codes + event_codes\n",
    "\n",
    "#other_important = ['actor1_name', 'actor2_name', 'actor1_geo_full_name','actor2_geo_fullname']\n",
    "\n",
    "actors = pd.read_csv(\"Entity_Codes/actor_codes.csv\")\n",
    "# countries = pd.read_csv(\"Entity_Codes/country_codes.csv\")\n",
    "events = pd.read_csv(\"Entity_Codes/event_codes.csv\")\n",
    "codes = pd.concat([actors, events], ignore_index=True)\n",
    "code_dictionary = dict(zip(codes['Code'], codes['Description']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the date range\n",
    "start_date = datetime.datetime(2023, 8, 13)\n",
    "end_date = datetime.datetime(2024, 8, 13)\n",
    "\n",
    "# Create a date range list\n",
    "date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "# Define the file path template\n",
    "file_path_template = \"./Data_Storage/GDELT Event Files/{date}.export.CSV\"\n",
    "\n",
    "describe_columns = []\n",
    "# Loop through each date in the range\n",
    "for single_date in date_range:\n",
    "    date_str = single_date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    # Construct the file path for the current date\n",
    "    file_path = file_path_template.format(date=date_str)\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Processing file: {file_path}\")\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "        df.columns = column_names\n",
    "\n",
    "        # Clean the dataframe\n",
    "        df_avg = clean_dataframe(df)\n",
    "        \n",
    "        # Apply the remove_duplicates_concatenate function\n",
    "        df = remove_duplicates_concatenate(df, important_columns)\n",
    "\n",
    "        # Fusionar los datos procesados con los promedios calculados\n",
    "        df = df.merge(df_avg, on='source_url', how='left')\n",
    "\n",
    "        # Agregar las descripciones\n",
    "        sorted_data = add_description(df, actor_codes, event_codes, code_dictionary)\n",
    "\n",
    "        describe_columns.append(sorted_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.concat(describe_columns, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
