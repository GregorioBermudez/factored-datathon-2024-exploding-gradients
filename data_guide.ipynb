{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide to the data\n",
    "In this notebook I will show you how to access the data wtih Pandas.\n",
    "Here you will find:\n",
    "- How to open the file\n",
    "- The correct column names\n",
    "- An explanation of some relevant column names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Reading the data\n",
    "You can use the data scraping script on a jupyter notebook or on a google colab to download the data. On google colab it runs quickly.\n",
    "After you download the data, to open the file without problems you need to do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\grego\\AppData\\Local\\Temp\\ipykernel_3912\\842848750.py:5: DtypeWarning: Columns (14,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('/Users/grego/OneDrive/Documentos/GitHub/factored-datathon-2024-exploding-gradients/Data_Storage/GDELT Event Files/20240813.export.CSV', sep = '\\t', header = None)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "df = pd.read_csv('/Users/grego/OneDrive/Documentos/GitHub/factored-datathon-2024-exploding-gradients/Data_Storage/GDELT Event Files/20240813.export.CSV', sep = '\\t', header = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Columns and explanation\n",
    "Here is a list containing all the column names and a brief explanation of the ones I found relevant. The dataset needs the column names to be assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['global_id',\n",
    " 'day', # Date the event took place in YYYYMMDD format\n",
    " 'month_year', # Alternative formating YYYYMM\n",
    " 'year', # Year\n",
    " 'fraction_date', # Alternative formating YYYY.FFFF, where FFFF is the percentage of the year completed by that day\n",
    "# actor 1\n",
    " 'actor1_code',\n",
    " 'actor1_name', # Name of Actor 1\n",
    " 'actor1_country_code',\n",
    " 'actor1_known_group_code', # Which group the actor belongs to NGO/ IGO/ rebel group. Ex: United Nations\n",
    " 'actor1_ethnic_code',\n",
    " 'actor1_religion1_code',\n",
    " 'actor1_religion2_code',\n",
    " 'actor1_type1_code', # Type codes talk about roles, for example police forces\n",
    " 'actor1_type2_code', # goverment, military, education, elites, media, etc\n",
    " 'actor1_type3_code', # -\n",
    "# actor 2\n",
    " 'actor2_code',\n",
    " 'actor2_name', # Name of actor 2\n",
    " 'actor2_country_code',\n",
    " 'actor2_known_group_code',\n",
    " 'actor2_ethnic_code',\n",
    " 'actor2_religion1_code',\n",
    " 'actor2_religion2_code',\n",
    " 'actor2_type1_code', # Same as in actor 1\n",
    " 'actor2_type2_code', # -\n",
    " 'actor2_type3_code', # -\n",
    "# ----------------\n",
    " 'is_root_event', # Binary. Says if it is the root event. Can give insight into importance\n",
    " 'event_code',\n",
    " 'event_base_code',\n",
    " 'event_root_code',\n",
    " 'quad_class', # Event taxonomy: 1. Verbal cooperation, 2. Material Cooperation, 3. Verbal Conflict, 4. Material Conflict\n",
    " 'goldstein_scale', # Numeric score from -10 to +10 capturing potential impact that the event will have in countries stability\n",
    " 'num_mentions', # Number of mentions of the event across all documents. Can be seen as importance measure\n",
    " 'num_sources', # Number of information sources containing mentions of the event\n",
    " 'num_articles',# Number of source documents containing mentions of this event\n",
    " 'avg_tone', # Avg tone of documents that mention the event. Goes from -100 (extremely negative) to 100 (extremely positive)\n",
    "# actor 1 geo\n",
    " 'actor1_geo_type', # Maps to: 1.Country, 2. US State, 3. US City, 4. World city, 5. World State\n",
    " 'actor1_geo_full_name', # Name of location\n",
    " 'actor1_geo_country_code',\n",
    " 'actor1_geo_adm1_code',\n",
    " 'actor1_geo_lat', # Latitude\n",
    " 'actor1_geo_long', # Longitude\n",
    " 'actor1_geo_feature_id',\n",
    "# actor 2 geo\n",
    " 'actor2_geo_type', # Check actor 1\n",
    " 'actor2_geo_fullname',\n",
    " 'actor2_geo_countrycode',\n",
    " 'actor2_geo_adm1_code',\n",
    " 'actor2_geo_lat',\n",
    " 'actor2_geo_long',\n",
    " 'actor2_geo_feature_id',\n",
    "# action geo\n",
    " 'action_geo_type', # Check actor 1\n",
    " 'action2_geo_full_name',\n",
    " 'action_geo_country_code',\n",
    " 'action_geo_adm1_code',\n",
    " 'action_geo_lat',\n",
    " 'action_geo_long',\n",
    " 'action_geo_feature_id',\n",
    "# date and url\n",
    " 'date_added', # Date the event was added to master database\n",
    " 'source_url'] # URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the data is ready and you can use it as you would normally with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before: 132404\n",
      "Rows after: 28639\n"
     ]
    }
   ],
   "source": [
    "# df = df.dropna(subset=['avg_tone', 'goldstein_scale', 'num_mentions', 'num_sources', 'num_articles'])\n",
    "\n",
    "# Deleating duplicated rows\n",
    "len_before = df\n",
    "len_after = df.drop_duplicates(subset=['source_url'])\n",
    "print('Rows before:', len(len_before))\n",
    "print('Rows after:', len(len_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most repeated URL is: https://www.theborneopost.com/2024/08/13/abg-jo-use-niah-caves-to-delve-deeper-into-history-of-human-settlement-in-borneo/ with 173 occurrences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_tone</th>\n",
       "      <th>goldstein_scale</th>\n",
       "      <th>num_mentions</th>\n",
       "      <th>num_sources</th>\n",
       "      <th>num_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>173.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>173.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.581913</td>\n",
       "      <td>2.494798</td>\n",
       "      <td>2.861272</td>\n",
       "      <td>1.011561</td>\n",
       "      <td>2.861272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.262376</td>\n",
       "      <td>0.068426</td>\n",
       "      <td>1.779519</td>\n",
       "      <td>0.107208</td>\n",
       "      <td>1.779519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.784529</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.597610</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.597610</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.597610</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.264252</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         avg_tone  goldstein_scale  num_mentions  num_sources  num_articles\n",
       "count  173.000000       173.000000    173.000000   173.000000    173.000000\n",
       "mean     0.581913         2.494798      2.861272     1.011561      2.861272\n",
       "std      0.262376         0.068426      1.779519     0.107208      1.779519\n",
       "min     -2.784529         1.900000      2.000000     1.000000      2.000000\n",
       "25%      0.597610         2.500000      2.000000     1.000000      2.000000\n",
       "50%      0.597610         2.500000      2.000000     1.000000      2.000000\n",
       "75%      0.597610         2.500000      4.000000     1.000000      4.000000\n",
       "max      1.264252         2.800000     20.000000     2.000000     20.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify the most repeated URL\n",
    "url_counts = df['source_url'].value_counts()\n",
    "most_repeated_url = url_counts.idxmax()\n",
    "repeat_count = url_counts.max()\n",
    "\n",
    "print(f\"The most repeated URL is: {most_repeated_url} with {repeat_count} occurrences.\")\n",
    "\n",
    "# Inspect variables associated with the most repeated URL\n",
    "most_repeated_url_data = df[df['source_url'] == most_repeated_url]\n",
    "most_repeated_url_data[['avg_tone', 'goldstein_scale', 'num_mentions', 'num_sources', 'num_articles']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (132404, 58)\n",
      "Combined DataFrame shape: (28639, 58)\n"
     ]
    }
   ],
   "source": [
    "# Define the columns to preserve the most common values\n",
    "columns_to_preserve = ['avg_tone', 'goldstein_scale', 'num_mentions', 'num_sources', 'num_articles']\n",
    "\n",
    "# Function to get the mode (most frequent value)\n",
    "def mode(series):\n",
    "    return series.mode()[0] if not series.mode().empty else series.iloc[0]\n",
    "\n",
    "# Group by 'source_url' and apply the mode function to the specified columns\n",
    "df_grouped = df.groupby('source_url').agg({col: mode for col in columns_to_preserve}).reset_index()\n",
    "\n",
    "# Merge the grouped data with other columns from the original DataFrame\n",
    "df_combined = pd.merge(df_grouped, df.drop(columns=columns_to_preserve).drop_duplicates('source_url'), on='source_url', how='left')\n",
    "\n",
    "# Check the shape of the original and combined DataFrames\n",
    "print('Original DataFrame shape:', df.shape)\n",
    "print('Combined DataFrame shape:', df_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial=df_combined.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import pandas as pd\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.log import configure_logging\n",
    "from twisted.internet.defer import inlineCallbacks, Deferred\n",
    "import nest_asyncio\n",
    "from newspaper import Article\n",
    "from twisted.internet import reactor\n",
    "\n",
    "# Apply the necessary fix for asyncio to work in Jupyter Notebooks\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class NewsSpider(scrapy.Spider):\n",
    "    name = \"news_spider\"\n",
    "\n",
    "    def __init__(self, urls, *args, **kwargs):\n",
    "        super(NewsSpider, self).__init__(*args, **kwargs)\n",
    "        self.urls = urls\n",
    "        self.extracted_texts = []\n",
    "\n",
    "    def start_requests(self):\n",
    "        for url in self.urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse, errback=self.errback_handler)\n",
    "\n",
    "    def parse(self, response):\n",
    "        try:\n",
    "            article = Article(response.url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            text = article.text\n",
    "            if text:\n",
    "                self.extracted_texts.append(text)\n",
    "            else:\n",
    "                self.extracted_texts.append(\"No content extracted\")\n",
    "            self.logger.info(f\"Extracted text for {response.url}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to extract text from {response.url} with error: {str(e)}\")\n",
    "            self.extracted_texts.append(\"Error during extraction\")\n",
    "\n",
    "    def errback_handler(self, failure):\n",
    "        self.logger.error(f\"Request failed for {failure.request.url} with error: {failure.value}\")\n",
    "        self.extracted_texts.append(\"Request failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = trial['source_url'].tolist()\n",
    "# Configure logging for Scrapy\n",
    "configure_logging()\n",
    "runner = CrawlerRunner()\n",
    "\n",
    "@inlineCallbacks\n",
    "def crawl():\n",
    "    spider = yield runner.crawl(NewsSpider, urls=urls)\n",
    "    reactor.stop()\n",
    "    return spider\n",
    "\n",
    "# Run the crawl process\n",
    "deferred = crawl()\n",
    "reactor.run()\n",
    "\n",
    "# Retrieve the spider instance and its extracted texts\n",
    "spider = deferred.result\n",
    "df['extracted_text'] = spider.extracted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['global_id', 'day', 'month_year', 'year', 'fraction_date',\n",
       "       'is_root_event', 'event_code', 'event_base_code', 'event_root_code',\n",
       "       'quad_class', 'goldstein_scale', 'num_mentions', 'num_sources',\n",
       "       'num_articles', 'avg_tone', 'actor1_geo_type', 'actor1_geo_lat',\n",
       "       'actor1_geo_long', 'actor2_geo_type', 'actor2_geo_lat',\n",
       "       'actor2_geo_long', 'action_geo_type', 'action_geo_lat',\n",
       "       'action_geo_long', 'date_added'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_numeric = df.select_dtypes(include=[float, int])\n",
    "df_numeric.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
