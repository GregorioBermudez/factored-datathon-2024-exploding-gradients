{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e698099-6721-4edf-b806-13a81c2c8d36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install scrapy\n",
    "# Databricks notebook source\n",
    "from pyspark.sql import SparkSession\n",
    "import builtins\n",
    "from pyspark.sql.functions import current_timestamp, to_date, mean, abs, lit, datediff, col\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, DoubleType\n",
    "from datetime import datetime\n",
    "import requests\n",
    "from scrapy import Selector\n",
    "import io\n",
    "import zipfile\n",
    "import tempfile\n",
    "import os\n",
    "# COMMAND ----------\n",
    "\n",
    "# Configuration\n",
    "bucket_name = \"datathonfactored2024\"\n",
    "events_folder = \"GDELT Event Files\"\n",
    "gkg_folder = \"GDELT GKG Files\"\n",
    "gkgcounts_folder = \"GDELT GKG Files/gkgcounts\"\n",
    "bronze_table_name = \"bronze_layer\"\n",
    "silver_table_name = \"silver_layer\"\n",
    "gold_table_name = \"gold_layer\"\n",
    "gdelt_schema = StructType([\n",
    "    StructField(\"GlobalEventID\", IntegerType(), False),\n",
    "    StructField(\"Day\", IntegerType(), False),\n",
    "    StructField(\"MonthYear\", IntegerType(), False),\n",
    "    StructField(\"Year\", IntegerType(), False),\n",
    "    StructField(\"FractionDate\", DoubleType(), False),\n",
    "    StructField(\"Actor1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Name\", StringType(), True),\n",
    "    StructField(\"Actor1CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor1EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor1Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor1Type3Code\", StringType(), True),\n",
    "    StructField(\"Actor2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Name\", StringType(), True),\n",
    "    StructField(\"Actor2CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2KnownGroupCode\", StringType(), True),\n",
    "    StructField(\"Actor2EthnicCode\", StringType(), True),\n",
    "    StructField(\"Actor2Religion1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Religion2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type2Code\", StringType(), True),\n",
    "    StructField(\"Actor2Type3Code\", StringType(), True),\n",
    "    StructField(\"IsRootEvent\", IntegerType(), True),\n",
    "    StructField(\"EventCode\", StringType(), True),\n",
    "    StructField(\"EventBaseCode\", StringType(), True),\n",
    "    StructField(\"EventRootCode\", StringType(), True),\n",
    "    StructField(\"QuadClass\", IntegerType(), True),\n",
    "    StructField(\"GoldsteinScale\", DoubleType(), True),\n",
    "    StructField(\"NumMentions\", IntegerType(), True),\n",
    "    StructField(\"NumSources\", IntegerType(), True),\n",
    "    StructField(\"NumArticles\", IntegerType(), True),\n",
    "    StructField(\"AvgTone\", DoubleType(), True),\n",
    "    StructField(\"Actor1Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor1Geo_Fullname\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor1Geo_Lat\", DoubleType(), True),\n",
    "    StructField(\"Actor1Geo_Long\", DoubleType(), True),\n",
    "    StructField(\"Actor1Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Type\", IntegerType(), True),\n",
    "    StructField(\"Actor2Geo_Fullname\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_CountryCode\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"Actor2Geo_Lat\", DoubleType(), True),\n",
    "    StructField(\"Actor2Geo_Long\", DoubleType(), True),\n",
    "    StructField(\"Actor2Geo_FeatureID\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Type\", IntegerType(), True),\n",
    "    StructField(\"ActionGeo_Fullname\", StringType(), True),\n",
    "    StructField(\"ActionGeo_CountryCode\", StringType(), True),\n",
    "    StructField(\"ActionGeo_ADM1Code\", StringType(), True),\n",
    "    StructField(\"ActionGeo_Lat\", DoubleType(), True),\n",
    "    StructField(\"ActionGeo_Long\", DoubleType(), True),\n",
    "    StructField(\"ActionGeo_FeatureID\", StringType(), True),\n",
    "    StructField(\"DATEADDED\", IntegerType(), False),\n",
    "    StructField(\"SOURCEURL\", StringType(), False)\n",
    "])\n",
    "# COMMAND ----------\n",
    "\n",
    "def extract_date(filename):\n",
    "    try:\n",
    "        date_str = filename.split('.')[0].split('/')[-1]\n",
    "        return datetime.strptime(date_str, \"%Y%m%d\")\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def get_most_recent_date(folder_name):\n",
    "    files = dbutils.fs.ls(f\"s3a://{bucket_name}/{folder_name}\")\n",
    "    dates = [extract_date(file.name) for file in files if extract_date(file.name)]\n",
    "    return max(dates) if dates else datetime(1970, 1, 1)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def calculate_importance(df):\n",
    "    \"\"\"\n",
    "    Calculates the importance score for each row in the dataframe.\n",
    "    \"\"\"\n",
    "    window = Window.partitionBy()\n",
    "    max_num_sources = df.select(F.max(\"AvgNumSources\")).first()[0]\n",
    "    max_abs_tone = df.select(F.max(abs(\"AvgAvgTone\"))).first()[0]\n",
    "    \n",
    "    return df.withColumn(\n",
    "        \"Importance\",\n",
    "        (2 * col(\"AvgNumSources\") / lit(max_num_sources)) + \n",
    "        (abs(col(\"AvgAvgTone\")) / lit(max_abs_tone))\n",
    "    )\n",
    "# COMMAND ----------\n",
    "\n",
    "def fetch_new_files(start_date):\n",
    "    new_files = {'events': []}    \n",
    "\n",
    "    events_url = \"http://data.gdeltproject.org/events/index.html\"\n",
    "    base = \"http://data.gdeltproject.org/events/\"\n",
    "    response = requests.get(events_url)\n",
    "    sel = Selector(text=response.text)\n",
    "    links = sel.xpath('//a/@href').extract()\n",
    "    \n",
    "    for link in links:\n",
    "        if link.endswith('.zip'):\n",
    "            date_str = link.split('.')[0]\n",
    "            try:\n",
    "                file_date = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "                if file_date > start_date:\n",
    "                    new_files['events'].append(base + link)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    # Uncomment and modify this section if you want to include GKG files\n",
    "    # gkg_url = 'http://data.gdeltproject.org/gkg/index.html'    \n",
    "    # base = \"http://data.gdeltproject.org/gkg/\"\n",
    "    # response = requests.get(gkg_url)\n",
    "    # sel = Selector(text=response.text)\n",
    "    # links = sel.xpath('//a/@href').extract()   \n",
    "\n",
    "    # for link in links:\n",
    "    #     if link.endswith('.zip'):\n",
    "    #         date_str = link.split('.')[0]\n",
    "    #         try:\n",
    "    #             file_date = datetime.strptime(date_str, \"%Y%m%d\")\n",
    "    #             if file_date > start_date:\n",
    "    #                 file_type = 'gkgcounts' if 'gkgcounts' in link else 'gkg'\n",
    "    #                 new_files[file_type].append(base + link)\n",
    "    #         except ValueError:\n",
    "    #             continue     \n",
    "\n",
    "    return new_files\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def ingest_to_bronze(file_path):\n",
    "    df = spark.read.csv(file_path, schema=gdelt_schema, sep=\"\\t\")\n",
    "    df = df.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "    \n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(bronze_table_name)\n",
    "    print(f\"Ingested {file_path} to bronze table: {bronze_table_name}\")\n",
    "\n",
    "def process_to_silver():\n",
    "    bronze_df = spark.table(bronze_table_name)\n",
    "    \n",
    "    silver_df = bronze_df.select(\n",
    "        \"Day\",\n",
    "        \"SOURCEURL\",\n",
    "        \"NumSources\",\n",
    "        \"AvgTone\",\n",
    "        \"GoldsteinScale\",\n",
    "        \"NumArticles\"\n",
    "    )\n",
    "    \n",
    "    silver_df = silver_df.groupBy(\"Day\", \"SOURCEURL\").agg(\n",
    "        mean(\"NumSources\").alias(\"AvgNumSources\"),\n",
    "        mean(\"AvgTone\").alias(\"AvgAvgTone\"),\n",
    "        mean(\"GoldsteinScale\").alias(\"AvgGoldsteinScale\"),\n",
    "        mean(\"NumArticles\").alias(\"AvgNumArticles\")\n",
    "    )\n",
    "    \n",
    "    silver_df = silver_df.withColumn(\"transform_timestamp\", current_timestamp())\n",
    "    \n",
    "    silver_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_table_name)\n",
    "    print(f\"Processed data to silver table: {silver_table_name}\")\n",
    "\n",
    "def process_to_gold():\n",
    "    \"\"\"\n",
    "    Transforms all data from Silver to Gold layer, adding the Importance score.\n",
    "    \"\"\"\n",
    "    # Read from Silver table\n",
    "    silver_df = spark.table(silver_table_name)\n",
    "    \n",
    "    # Calculate importance for all rows\n",
    "    gold_df = calculate_importance(silver_df)\n",
    "    \n",
    "    # Write to Gold Delta table\n",
    "    gold_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(gold_table_name)\n",
    "    \n",
    "    print(f\"All data transformed and saved to {gold_table_name}\")\n",
    "\n",
    "def process_new_files(new_files):\n",
    "    for file_type, links in new_files.items():\n",
    "        for link in links:\n",
    "            file_name = link.split('/')[-1]\n",
    "            response = requests.get(link)\n",
    "            file_content = io.BytesIO(response.content)\n",
    "            \n",
    "            with zipfile.ZipFile(file_content, 'r') as zip_ref:\n",
    "                for file_info in zip_ref.infolist():\n",
    "                    date = file_info.filename.split('.')[0]\n",
    "\n",
    "                    if file_type == 'events':\n",
    "                        folder_name = events_folder\n",
    "                        new_file_name = f\"{date}.export.csv\"\n",
    "                    elif file_type == 'gkg':\n",
    "                        folder_name = gkg_folder\n",
    "                        new_file_name = f\"{date}.{file_type}.csv\"\n",
    "                    elif file_type == 'gkgcounts':\n",
    "                        folder_name = gkgcounts_folder\n",
    "                        new_file_name = f\"{date}.{file_type}.csv\"\n",
    "\n",
    "                    s3_path = f\"/dbfs/mnt/{bucket_name}/{folder_name}/{new_file_name}\"\n",
    "                    \n",
    "                    with zip_ref.open(file_info) as extracted_file:\n",
    "                        # Create a temporary file\n",
    "                        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "                            temp_file.write(extracted_file.read())\n",
    "                            temp_file_path = temp_file.name\n",
    "\n",
    "                    # Use dbutils to copy the temp file to S3\n",
    "                    dbutils.fs.cp(f\"file:{temp_file_path}\", f\"dbfs:/mnt/{bucket_name}/{folder_name}/{new_file_name}\")\n",
    "                    \n",
    "                    # Remove the temporary file\n",
    "                    os.unlink(temp_file_path)\n",
    "\n",
    "                    print(f\"Uploaded {new_file_name} to S3 path: s3a://{bucket_name}/{folder_name}/{new_file_name}\")\n",
    "                    ingest_to_bronze(f\"dbfs:/mnt/{bucket_name}/{folder_name}/{new_file_name}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Main execution\n",
    "start_date = get_most_recent_date(events_folder)\n",
    "new_files = fetch_new_files(start_date)\n",
    "process_new_files(new_files)\n",
    "\n",
    "# COMMAND ----------\n",
    "process_to_silver()\n",
    "process_to_gold()\n",
    "print(\"Data ingestion complete.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "new_data_aws",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
