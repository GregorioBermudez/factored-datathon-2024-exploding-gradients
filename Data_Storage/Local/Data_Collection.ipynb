{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gZ5faohzchlO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from scrapy import Selector\n",
        "from datetime import datetime\n",
        "\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuZiOTDqchlQ"
      },
      "source": [
        "## GDELT 1.0 GKG Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Vu_rx6HGchlR"
      },
      "outputs": [],
      "source": [
        "gkg_url = 'http://data.gdeltproject.org/gkg/index.html'\n",
        "response = requests.get(gkg_url)\n",
        "sel = Selector(text = response.text)\n",
        "\n",
        "start_date = datetime(2023, 8, 13)\n",
        "end_date = datetime(2024, 8, 13)\n",
        "\n",
        "# Get the links as a url that can be downloaded later\n",
        "links = sel.xpath('//a/@href').extract()\n",
        "\n",
        "gkg_links = []\n",
        "gkgcounts_links = []\n",
        "base = \"http://data.gdeltproject.org/gkg/\"\n",
        "\n",
        "for link in links:\n",
        "    if link.endswith('.zip'):\n",
        "        date_str = link.split('.')[0]\n",
        "        try:\n",
        "            file_date = datetime.strptime(date_str, \"%Y%m%d\")\n",
        "            if start_date <= file_date <= end_date:\n",
        "                if 'gkgcounts' in link:\n",
        "                    gkgcounts_links.append(base + link)\n",
        "                else:\n",
        "                    gkg_links.append(base + link)\n",
        "        except ValueError:\n",
        "            continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "destination = \"GDELT GKG Files\"\n",
        "\n",
        "os.makedirs(destination, exist_ok=True)\n",
        "\n",
        "for link in gkg_links:\n",
        "    file_name = link.split('/')[-1]\n",
        "    file_path = os.path.join(destination, file_name)\n",
        "\n",
        "    response = requests.get(link)\n",
        "\n",
        "    with open(file_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "    # Decompress the zip file\n",
        "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(destination)\n",
        "        print(f\"Extracted {file_name} to {destination}\")\n",
        "\n",
        "    # Delete the zip file after extraction\n",
        "    os.remove(file_path)\n",
        "\n",
        "print(\"All files downloaded and extracted.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "destination = \"GDELT GKG Files/gkgcounts\"\n",
        "\n",
        "os.makedirs(destination, exist_ok=True)\n",
        "\n",
        "for link in gkgcounts_links:\n",
        "    file_name = link.split('/')[-1]\n",
        "    file_path = os.path.join(destination, file_name)\n",
        "\n",
        "    response = requests.get(link)\n",
        "\n",
        "    with open(file_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "    # Decompress the zip file\n",
        "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(destination)\n",
        "        print(f\"Extracted {file_name} to {destination}\")\n",
        "\n",
        "    # Delete the zip file after extraction\n",
        "    os.remove(file_path)\n",
        "\n",
        "print(\"All files downloaded and extracted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugFOICLochlS"
      },
      "source": [
        "## GDELT 1.0 Events Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3sBZCFtdchlS"
      },
      "outputs": [],
      "source": [
        "events_url = \"http://data.gdeltproject.org/events/index.html\"\n",
        "response = requests.get(events_url)\n",
        "sel = Selector(text = response.text)\n",
        "\n",
        "start_date = datetime(2023, 8, 13)\n",
        "end_date = datetime(2024, 8, 13)\n",
        "\n",
        "# Get the links as a url that can be downloaded later\n",
        "links = sel.xpath('//a/@href').extract()\n",
        "\n",
        "downloadable_links = []\n",
        "base = \"http://data.gdeltproject.org/events/\"\n",
        "for link in links:\n",
        "    if link.endswith('.zip'):\n",
        "        date_str = link.split('.')[0]\n",
        "        try:\n",
        "            file_date = datetime.strptime(date_str, \"%Y%m%d\")\n",
        "            if start_date <= file_date <= end_date:\n",
        "                downloadable_links.append(base + link)\n",
        "        except ValueError:\n",
        "            continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQqUTm5NchlT",
        "outputId": "d616ac22-9543-4f73-98e7-6ed336bdfc3c"
      },
      "outputs": [],
      "source": [
        "destination = \"GDELT Event Files\"\n",
        "\n",
        "os.makedirs(destination, exist_ok=True)\n",
        "\n",
        "for link in downloadable_links:\n",
        "    file_name = link.split('/')[-1]\n",
        "    file_path = os.path.join(destination, file_name)\n",
        "\n",
        "    response = requests.get(link)\n",
        "\n",
        "    with open(file_path, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "\n",
        "    # Decompress the zip file\n",
        "    with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(destination)\n",
        "        print(f\"Extracted {file_name} to {destination}\")\n",
        "\n",
        "    # Delete the zip file after extraction\n",
        "    os.remove(file_path)\n",
        "\n",
        "print(\"All files downloaded and extracted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "bFVH29jrgvyg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/_s/n53dl30x6x90zrn97fjh8_6m0000gn/T/ipykernel_9080/2165017521.py:1: DtypeWarning: Columns (14,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  trial = pd.read_csv(\"./GDELT Event Files/20240813.export.CSV\", sep= \"\\t\", header = None)\n"
          ]
        }
      ],
      "source": [
        "trial = pd.read_csv(\"./GDELT Event Files/20240813.export.CSV\", sep= \"\\t\", header = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "v6CwelPopGZK"
      },
      "outputs": [],
      "source": [
        "csv_columns = ['global_id',\n",
        " 'day', # Date the event took place in YYYYMMDD format\n",
        " 'month_year', # Alternative formating YYYYMM\n",
        " 'year', # Year\n",
        " 'fraction_date', # Alternative formating YYYY.FFFF, where FFFF is the percentage of the year completed by that day\n",
        "# actor 1\n",
        " 'actor1_code',\n",
        " 'actor1_name', # Name of Actor 1\n",
        " 'actor1_country_code',\n",
        " 'actor1_known_group_code', # Which group the actor belongs to NGO/ IGO/ rebel group. Ex: United Nations\n",
        " 'actor1_ethnic_code',\n",
        " 'actor1_religion1_code',\n",
        " 'actor1_religion2_code',\n",
        " 'actor1_type1_code', # Type codes talk about roles, for example police forces\n",
        " 'actor1_type2_code', # goverment, military, education, elites, media, etc\n",
        " 'actor1_type3_code', # -\n",
        "# actor 2\n",
        " 'actor2_code',\n",
        " 'actor2_name', # Name of actor 2\n",
        " 'actor2_country_code',\n",
        " 'actor2_known_group_code',\n",
        " 'actor2_ethnic_code',\n",
        " 'actor2_religion1_code',\n",
        " 'actor2_religion2_code',\n",
        " 'actor2_type1_code', # Same as in actor 1\n",
        " 'actor2_type2_code', # -\n",
        " 'actor2_type3_code', # -\n",
        "# ----------------\n",
        " 'is_root_event', # Binary. Says if it is the root event. Can give insight into importance\n",
        " 'event_code',\n",
        " 'event_base_code',\n",
        " 'event_root_code',\n",
        " 'quad_class', # Event taxonomy: 1. Verbal cooperation, 2. Material Cooperation, 3. Verbal Conflict, 4. Material Conflict\n",
        " 'goldstein_scale', # Numeric score from -10 to +10 capturing potential impact that the event will have in countries stability\n",
        " 'num_mentions', # Number of mentions of the event across all documents. Can be seen as importance measure\n",
        " 'num_sources', # Number of information sources containing mentions of the event\n",
        " 'num_articles',# Number of source documents containing mentions of this event\n",
        " 'avg_tone', # Avg tone of documents that mention the event. Goes from -100 (extremely negative) to 100 (extremely positive)\n",
        "# actor 1 geo\n",
        " 'actor1_geo_type', # Maps to: 1.Country, 2. US State, 3. US City, 4. World city, 5. World State\n",
        " 'actor1_geo_full_name', # Name of location\n",
        " 'actor1_geo_country_code',\n",
        " 'actor1_geo_adm1_code',\n",
        " 'actor1_geo_lat', # Latitude\n",
        " 'actor1_geo_long', # Longitude\n",
        " 'actor1_geo_feature_id',\n",
        "# actor 2 geo\n",
        " 'actor2_geo_type', # Check actor 1\n",
        " 'actor2_geo_fullname',\n",
        " 'actor2_geo_countrycode',\n",
        " 'actor2_geo_adm1_code',\n",
        " 'actor2_geo_lat',\n",
        " 'actor2_geo_long',\n",
        " 'actor2_geo_feature_id',\n",
        "# action geo\n",
        " 'action_geo_type', # Check actor 1\n",
        " 'action2_geo_full_name',\n",
        " 'action_geo_country_code',\n",
        " 'action_geo_adm1_code',\n",
        " 'action_geo_lat',\n",
        " 'action_geo_long',\n",
        " 'action_geo_feature_id',\n",
        "# date and url\n",
        " 'date_added', # Date the event was added to master database\n",
        " 'source_url'] # URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QPCQ1HBfj9Vh"
      },
      "outputs": [],
      "source": [
        "trial.columns = csv_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/_s/n53dl30x6x90zrn97fjh8_6m0000gn/T/ipykernel_9080/3341980572.py:2: DtypeWarning: Columns (14,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(f\"GDELT Event Files/2024081{i}.export.CSV\", sep= \"\\t\", header = None)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(69353, 58)\n",
            "(116950, 58)\n"
          ]
        }
      ],
      "source": [
        "for i in range(1,3):\n",
        "    df = pd.read_csv(f\"GDELT Event Files/2024081{i}.export.CSV\", sep= \"\\t\", header = None)\n",
        "    df.columns = csv_columns\n",
        "    print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "actor2_type3_code\n",
              "MED    20\n",
              "COP    13\n",
              "GOV     8\n",
              "MIL     6\n",
              "ELI     5\n",
              "BUS     4\n",
              "EDU     3\n",
              "ENV     2\n",
              "JUD     1\n",
              "LEG     1\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.iloc[:,24].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from newspaper import Article\n",
        "import spacy\n",
        "\n",
        "# Initialize spacy NER model\n",
        "    # nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# List of URLs\n",
        "# urls = [\n",
        "#     \"https://theafronews.com/price-fixing-accusations-cast-shadow-on-food-industry-giants/\",\n",
        "#     \"https://theafronews.com/price-fixing-accusations-cast-shadow-on-food-industry-giants/\",\n",
        "#     \"https://www.wkrb13.com/2023/08/12/ingalls-snyder-llc-sells-8972-shares-of-bank-of-america-co-nysebac.html\",\n",
        "#     \"https://www.thetimes.co.uk/article/questions-over-who-really-owns-ppe-firm-linked-to-mone-given-200m-0t3vk5pbl\"\n",
        "# ]\n",
        "\n",
        "# Function to extract content from a URL\n",
        "def extract_article_text(url):\n",
        "    article = Article(url)\n",
        "    article.download()\n",
        "    try:\n",
        "        article.parse()\n",
        "    except:\n",
        "        pass\n",
        "    return article.title\n",
        "\n",
        "# Function to perform NER on the article title\n",
        "    def perform_ner(text):\n",
        "        doc = nlp(text)\n",
        "        return [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "# Process each URL\n",
        "# for url in urls:\n",
        "#     title, text = extract_article_text(url)\n",
        "#     entities = perform_ner(title)\n",
        "#     print(f\"Title: {title}\")\n",
        "#     print(f\"Entities: {entities}\")\n",
        "#     print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prueba = df[:100]\n",
        "prueba['titulo'] = prueba['source_url'].apply(extract_article_text)\n",
        "prueba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(r\"masterData.csv\")\n",
        "\n",
        "df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
